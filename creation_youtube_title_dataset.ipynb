{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: Khan Academy\n",
      "Processing channel: Y Combinator\n",
      "Processing channel: TED\n",
      "Processing channel: National Geographic\n",
      "Processing channel: SciShow\n",
      "Processing channel: Crash Course\n",
      "Processing channel: MIT OpenCourseWare\n",
      "Processing channel: BBC News\n",
      "Processing channel: The Economist\n",
      "Processing channel: Veritasium\n",
      "Processing channel: PBS NewsHour\n",
      "Processing channel: Financial Times\n",
      "Processing channel: PewDiePie\n",
      "Processing channel: MrBeast\n",
      "Processing channel: Markiplier\n",
      "Processing channel: Logan Paul\n",
      "Processing channel: KSI\n",
      "Processing channel: Dude Perfect\n",
      "Processing channel: James Charles\n",
      "Processing channel: David Dobrik\n",
      "Processing channel: Sidemen\n",
      "Processing channel: Surfing With Noz\n",
      "Processing channel: Arte\n",
      "Processing channel: France 24\n",
      "Processing channel: Le Monde\n",
      "Processing channel: Data Gueule\n",
      "Processing channel: Science Étonnante\n",
      "Processing channel: E-penser\n",
      "Processing channel: Doc Seven\n",
      "Processing channel: Nota Bene\n",
      "Processing channel: Squeezie\n",
      "Processing channel: Cyprien\n",
      "Processing channel: Norman\n",
      "Processing channel: Amixem\n",
      "Processing channel: Tibo InShape\n",
      "Processing channel: Léna Situations\n",
      "Processing channel: Michou\n",
      "Processing channel: Domingo\n",
      "Processing channel: Inoxtag 2.0\n",
      "Dataset created with 1950 videos\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "def get_channel_id(channel_name):\n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"q\": channel_name,\n",
    "        \"type\": \"channel\",\n",
    "        \"part\": \"id\"\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    return response.json()[\"items\"][0][\"id\"][\"channelId\"]\n",
    "\n",
    "def get_channel_videos(channel_id):\n",
    "    videos_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"channelId\": channel_id,\n",
    "        \"part\": \"snippet\",\n",
    "        \"order\": \"date\",\n",
    "        \"maxResults\": 50,\n",
    "        \"type\": \"video\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(videos_url, params=params)\n",
    "    return response.json()[\"items\"]\n",
    "\n",
    "# Define your channels and their types\n",
    "channels_data = [\n",
    "    # Add more channels as needed\n",
    "]\n",
    "\n",
    "channels_data = [\n",
    "    # English Informative Channels\n",
    "    {\"name\": \"Khan Academy\", \"is_informative\": 1},\n",
    "    {\"name\": \"Y Combinator\", \"is_informative\": 1},\n",
    "    {\"name\": \"TED\", \"is_informative\": 1},\n",
    "    {\"name\": \"National Geographic\", \"is_informative\": 1},\n",
    "    {\"name\": \"SciShow\", \"is_informative\": 1},\n",
    "    {\"name\": \"Crash Course\", \"is_informative\": 1},\n",
    "    {\"name\": \"MIT OpenCourseWare\", \"is_informative\": 1},\n",
    "    {\"name\": \"BBC News\", \"is_informative\": 1},\n",
    "    {\"name\": \"The Economist\", \"is_informative\": 1},\n",
    "    {\"name\": \"Veritasium\", \"is_informative\": 1},\n",
    "    {\"name\": \"PBS NewsHour\", \"is_informative\": 1},\n",
    "    {\"name\": \"Financial Times\", \"is_informative\": 1},\n",
    "    \n",
    "    # English Non-Informative Channels\n",
    "    {\"name\": \"PewDiePie\", \"is_informative\": 0},\n",
    "    {\"name\": \"MrBeast\", \"is_informative\": 0},\n",
    "    {\"name\": \"Markiplier\", \"is_informative\": 0},\n",
    "    {\"name\": \"Logan Paul\", \"is_informative\": 0},\n",
    "    {\"name\": \"KSI\", \"is_informative\": 0},\n",
    "    {\"name\": \"Dude Perfect\", \"is_informative\": 0},\n",
    "    {\"name\": \"James Charles\", \"is_informative\": 0},\n",
    "    {\"name\": \"David Dobrik\", \"is_informative\": 0},\n",
    "    {\"name\": \"Sidemen\", \"is_informative\": 0},\n",
    "    {\"name\": \"Surfing With Noz\", \"is_informative\": 0},\n",
    "    \n",
    "    # French Informative Channels\n",
    "    {\"name\": \"Arte\", \"is_informative\": 1},\n",
    "    {\"name\": \"France 24\", \"is_informative\": 1},\n",
    "    {\"name\": \"Le Monde\", \"is_informative\": 1},\n",
    "    {\"name\": \"Data Gueule\", \"is_informative\": 1},\n",
    "    {\"name\": \"Science Étonnante\", \"is_informative\": 1},\n",
    "    {\"name\": \"E-penser\", \"is_informative\": 1},\n",
    "    {\"name\": \"Doc Seven\", \"is_informative\": 1},\n",
    "    {\"name\": \"Nota Bene\", \"is_informative\": 1},\n",
    "    \n",
    "    # French Non-Informative Channels\n",
    "    {\"name\": \"Squeezie\", \"is_informative\": 0},\n",
    "    {\"name\": \"Cyprien\", \"is_informative\": 0},\n",
    "    {\"name\": \"Norman\", \"is_informative\": 0},\n",
    "    {\"name\": \"Amixem\", \"is_informative\": 0},\n",
    "    {\"name\": \"Tibo InShape\", \"is_informative\": 0},\n",
    "    {\"name\": \"Léna Situations\", \"is_informative\": 0},\n",
    "    {\"name\": \"Michou\", \"is_informative\": 0},\n",
    "    {\"name\": \"Domingo\", \"is_informative\": 0},\n",
    "    {\"name\": \"Inoxtag 2.0\", \"is_informative\": 0}\n",
    "]\n",
    "\n",
    "# Create empty lists to store data\n",
    "all_titles = []\n",
    "all_labels = []\n",
    "\n",
    "# Collect data from each channel\n",
    "for channel in channels_data:\n",
    "    try:\n",
    "        print(f\"Processing channel: {channel['name']}\")\n",
    "        channel_id = get_channel_id(channel['name'])\n",
    "        videos = get_channel_videos(channel_id)\n",
    "        \n",
    "        # Extract titles and add to lists\n",
    "        for video in videos:\n",
    "            title = video[\"snippet\"][\"title\"]\n",
    "            all_titles.append(title)\n",
    "            all_labels.append(channel['is_informative'])\n",
    "        \n",
    "        # Add a delay to avoid hitting API rate limits\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {channel['name']}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'title': all_titles,\n",
    "    'is_informative': all_labels\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('youtube_titles_dataset.csv', index=False)\n",
    "print(f\"Dataset created with {len(df)} videos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
